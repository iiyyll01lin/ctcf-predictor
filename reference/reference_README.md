CTCF Binding Site Prediction Pipeline - 強化版
專案目標
此專案旨在預測 DNA 序列中的 CTCF (CCCTC-binding factor) 轉錄因子潛在結合位點。CTCF 在染色質組織和基因表達調控中扮演關鍵角色。本強化版旨在透過引入更嚴謹的資料切分策略及模型評估方法，確保模型具備真實的泛化能力，避免性能過度高估的問題 [Schreiber et al. 2020, 4, 10, LOCO-EPI 2024, 504, 509]。
核心原則：避免資訊洩漏，追求真實泛化能力
在基因組預測任務中，尤其是像 CTCF 結合預測這類問題，最關鍵的挑戰在於確保模型能夠泛化到從未見過的基因組區域 [Schreiber et al. 2020, 4]。如果訓練集和測試集之間存在基因組區域的重疊，即使模型沒有真正學習到生物學規律，它也可能透過「記憶」這些重疊區域的特徵來表現出虛假的高性能 [Schreiber et al. 2020, 2, 10]。這種「資訊洩漏」會導致模型性能被過度高估 [LOCO-EPI 2024, 504, 509]。
CTCF 作為一個在不同細胞類型中具有相似結合模式的轉錄因子，特別容易受到這種「記憶」效應的影響 [Schreiber et al. 2020, 8, 11]。
關鍵改進概述
本強化版 pipeline 著重於以下主要改進，以解決先前 README.md 檔中隨機資料切分可能導致的性能過度高估問題 [Schreiber et al. 2020, 2, 10, LOCO-EPI 2024, 504, 509]：
•
採納 LOCO (Leave-One-Chromosome-Out) 資料切分策略：這是最嚴謹且推薦的資料切分方法，確保訓練集和測試集之間不存在基因組區域重疊，從而真實評估模型的泛化能力 [Schreiber et al. 2020, 4, Avsec et al. 2021, 41, 44, Kopp et al. 2020, 332, NetTIME 2022, 551, Toneyan et al. 2023, 397, LOCO-EPI 2024, 473, 505]。
•
從 PWM 模型轉向深度學習模型：對於複雜的基因組預測任務，深度學習模型（特別是卷積神經網絡 CNNs）被認為是更強大的選擇，能夠學習更複雜的序列特徵和協同作用 [Avsec et al. 2021, 31, 32, 34, 39, 40, Basset 2016, 245, 249, 250, 251, Kopp et al. 2020, 314, 321, 323, 328, NetTIME 2022, 531, 535]。
•
引入定量模型 (Quantitative Models)：相較於僅預測二元結合的分類模型，直接預測實驗讀取覆蓋值的定量模型（如 BPNet）通常能提供更好的泛化能力和可解釋性，因為它們保留了峰值形狀和幅度的資訊 [Avsec et al. 2021, 39, 42, Toneyan et al. 2023, 361, 376]。
•
加強模型解釋性：利用 DeepLIFT 和 TF-MoDISco 等工具，從訓練好的深度學習模型中提取 CTCF 結合 motif 及其「軟性 motif 語法」 [Avsec et al. 2021, 35, 36, 44, 47, 50, 58, 66, 67, 89, 93, 94]。
預測 Pipeline 概覽 (強化版)
以下是結合 README.md 檔的實用腳本與深度學習最佳實踐的整合流程：
0. 序列預處理 ()
•
目的：確保輸入序列的品質，提升模型效能。
•
執行腳本：scripts/preprocess_sequences.R。
•
主要功能：
◦
長度過濾與標準化。
◦
N-鹼基處理 (移除或替換)。
◦
低複雜度區域檢測與過濾。
◦
重複序列遮罩 (同型聚合物和簡單重複序列)。
•
說明：此步驟可透過命令列參數或設定檔自訂。
1. 資料收集與初始提取 ()
•
目的：獲取 CTCF 的 ChIP-seq 峰值數據（例如 BED 格式）和相應的參考基因組序列。
•
執行腳本：scripts/download_data.sh。
•
強化建議：
◦
考慮獲取 ChIP-nexus 數據，其鹼基對解析度對於學習精確的 motif 語法和 TF 協同作用更有幫助 [Avsec et al. 2021, 32, 33, 37, 49, 89, 99, 100, 104, 105, 111, 114, 116, 117, 169, 173, 175, NetTIME 2022, 567]。
◦
提取這些峰值區域及其周圍（例如，1kb 或更長）的 DNA 序列 [Avsec et al. 2021, 39, 40, NetTIME 2022, 544, Zhou & Troyanskaya 2015, 620, 640]。較長的序列上下文可以顯著提高模型性能 [Avsec et al. 2021, 40, 220, Zhou & Troyanskaya 2015, 621]。
◦
移除基因組中的黑名單區域（blacklisted regions），以避免假陽性 [Avsec et al. 2021, 115, 116, 147, Kopp et al. 2020, 337]。
2. 資料切分 ()
•
目的：將數據劃分為訓練、驗證和測試集，並生成負樣本。
•
執行腳本：scripts/prepare_datasets.R。
•
核心強化：實施 LOCO (Leave-One-Chromosome-Out) 交叉驗證 [Schreiber et al. 2020, 4, Avsec et al. 2021, 41, 44, Kopp et al. 2020, 332, NetTIME 2022, 551, Toneyan et al. 2023, 397, LOCO-EPI 2024, 473, 505]。
◦
實施方式：將所有基因組位點依染色體進行分割以進行交叉驗證 [Schreiber et al. 2020, 4, Avsec et al. 2021, 41, 44, Kopp et al. 2020, 332, NetTIME 2022, 551, Toneyan et al. 2023, 397, LOCO-EPI 2024, 473, 505]。例如，在一次迭代中，將某條染色體（例如 Chr1）上的所有 CTCF 結合位點（及其對應的負樣本）作為測試集，而將其餘染色體的所有資料用於訓練集 [LOCO-EPI 2024, 481]。
◦
驗證集：您可以從訓練集中再劃分出額外的一條或多條染色體作為驗證集，用於模型超參數調整和早期停止 [Avsec et al. 2021, 41, 128, Kopp et al. 2020, 340, Toneyan et al. 2023, 397, 398, Zhou & Troyanskaya 2015, 641]。
◦
負樣本生成：在確定好染色體分割後，再利用 prepare_datasets.R 提供的功能，在每個訓練/驗證/測試分割內部生成負樣本。推薦使用二核苷酸打亂 (Dinucleotide shuffling)，因為它能更好地保留序列複雜性 [603, 606, Avsec et al. 2021, 136, Toneyan et al. 2023, 423]。
◦
數據編碼：將 DNA 序列轉換為機器學習模型可處理的格式，例如 one-hot encoding [Avsec et al. 2021, 118, Basset 2016, 251, Kopp et al. 2020, 321, 323, Toneyan et al. 2023, 417]。
◦
優化建議：考慮使用更高階的序列特徵編碼（例如二核苷酸或三核苷酸編碼），這已被證實能顯著改善某些轉錄因子結合預測的性能，因為它直接捕獲了相鄰核苷酸之間的相關性 [Kopp et al. 2020, 322, 323, 324, 330, 334]。
3. 模型選擇與建構 ()
•
目的：建立用於預測 CTCF 結合的序列模型。
•
README.md 現有方案：創建 PWM。
•
核心強化：推薦使用深度學習模型 (CNNs) [Avsec et al. 2021, 31, 34, Basset 2016, 245, 249, 250, Kopp et al. 2020, 314, 323, NetTIME 2022, 531, 535, Toneyan et al. 2023, 375, Zhou & Troyanskaya 2015, 618, 634]。
◦
定量模型 (Quantitative Models)：相較於僅預測二元結合（有或無）的分類模型，直接預測實驗讀取覆蓋值的定量模型（如 BPNet）通常能提供更好的泛化能力和可解釋性 [Avsec et al. 2021, 39, 42, 55, 99, 100, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 138, 139, 140, 141, Toneyan et al. 2023, 361, 362, 365, 367, 368, 371, 376, 377, 378, 388, 389, 390, 391, 392, 401, 402, 403, 412, 413, 414, 415, 416, 417, 418, 419, 420, 429, 430, 431, 432, 434, 435, 436, 437, 438, 439, 458, 459, 460, 461, 463, 464, 465, 466, 467, 468, 469, 470]。它們能夠保留峰值形狀和幅度的信息，這對理解更精確的結合機制至關重要 [Toneyan et al. 2023, 373]。
◦
混合/多任務架構：可以考慮結合傳統 k-mer 特徵與深度學習的混合架構（如 LOCO-EPI 中的 MHybrid），這有助於降低對特定核苷酸序列的過度擬合 [LOCO-EPI 2024, 473, 479, 486, 487, 488, 489, 494, 497, 498, 499, 500, 501, 502, 504]。如果數據允許，同時預測多個轉錄因子或細胞類型的結合，已被證明能提高模型的效率和泛化能力 [NetTIME 2022, 530, 534, 538, 539, 540, 559, 560, 561, 566]。
4. 模型訓練 ()
•
目的：訓練模型學習 CTCF 結合模式。
•
執行：使用 LOCO 切分後的訓練集進行訓練。
•
強化建議：
◦
驗證集：利用 LOCO 切分出來的驗證染色體來監控模型訓練過程，調整超參數，以及執行**早期停止（early stopping）**以防止過度擬合 [Avsec et al. 2021, 41, 128, Kopp et al. 2020, 340, Toneyan et al. 2023, 397, 398, 409, 410, Zhou & Troyanskaya 2015, 641]。
◦
優化器與損失函數：常用 Adam 優化器 [Avsec et al. 2021, 128, Kopp et al. 2020, 342, 409, 550]。對於二元分類任務，使用二元交叉熵損失（binary cross-entropy loss）[LOCO-EPI 2024, 490]。對於定量預測任務，泊松負對數似然（Poisson NLL）通常表現良好 [Avsec et al. 2021, 122, 123, Toneyan et al. 2023, 402]。
◦
資料增強 (Data Augmentation)：在訓練過程中加入隨機平移 (random shifts) 和反向互補 (reverse-complement) 等資料增強策略，可以顯著提高模型的魯棒性（robustness）和變異效果預測的準確性 [Toneyan et al. 2023, 373, 374, 391, 409, 410, 411, 413, 462, 463, 467, 468]。
5. 模型評估 ()
•
目的：評估模型性能。
•
執行腳本：scripts/evaluate_models.R 或 scripts/evaluate_models_with_cv.R。
•
核心強化：在 LOCO 切分出的測試染色體上進行評估。這才是模型在未見過基因組區域上的真實泛化能力 [Schreiber et al. 2020, 4, LOCO-EPI 2024, 473, 478, 479, 492, 499, 506]。
◦
核心評估指標：主要使用 AUCROC (Area Under the Receiver Operating Characteristic curve) [LOCO-EPI 2024, 478, 491, 492, 493, 494, 495, 496, 497, 498, 499, 501, 504, NetTIME 2022, 551, Zhou & Troyanskaya 2015, 622, 630, 642, 670]。對於不平衡數據集，AUPRC (Area Under the Precision-Recall curve) 也是一個重要的指標 [Basset 2016, 253]。
◦
基準模型比較：將模型的性能與簡單的基準模型（如隨機猜測，AUC=0.5，或訓練集中的平均活性基準）進行比較 [Schreiber et al. 2020, 11, 12, LOCO-EPI 2024, 524]。
6. 閾值優化 ()
•
目的：確定最佳預測分數閾值。
•
執行腳本：scripts/optimize_threshold.R。
•
功能：支援多種優化策略，如 Youden's index、F1-score 等。
7. 在新序列上進行預測 ()
•
目的：使用訓練好的模型和優化後的閾值，掃描新的 DNA 序列以識別潛在的 CTCF 結合位點。
•
執行腳本：scripts/predict_ctcf.R。
8. 預測結果 ()
•
目的：輸出包含預測結合位點、序列名稱、座標、序列本身及預測分數的結構化文件。
9. 實驗驗證 ()
•
目的：確認模型預測的生物學意義 (概念性步驟 - 需要實驗室工作)。
•
強化建議：
◦
通過實驗（如 EMSA）驗證預測。
◦
評估模型預測單核苷酸變異（SNVs）對 CTCF 結合的影響，並與實驗數據（如 MPRA 數據）進行比較，這可以衡量模型在「分佈外」（out-of-distribution）數據上的泛化能力 [Toneyan et al. 2023, 379, 400, 401, 416, 417, 418, 419, 420]。
10. 模型解釋性 ()
•
目的：從訓練好的模型中提取 CTCF 結合 motif 和其相關的「soft motif syntax」等生物學洞察 [Avsec et al. 2021, 35, 36, 44, 47, 50, 58, 66, 67, 89, 93, 94]。
•
強化建議：
◦
利用深度學習模型的解釋工具（如 DeepLIFT、TF-MoDISco）[Avsec et al. 2021, 35, 36, 44, 47, 50, 58, 66, 67, 89, 93, 94, Toneyan et al. 2023, 380, 391, 421, 422, 423]。
◦
進行全局重要性分析 (Global Importance Analysis, GIA)，以定量測試特定序列特徵（如側翼核苷酸組合、不同 motif 之間的距離）對模型預測的影響 [Toneyan et al. 2023, 382, 383, 384, 385, 386, 387, 388, 389, 390, 423, 424, 425, 426, 427, 428, 432, 433, 434, 435, 436, 437, 438, 439, 468, 469, 470]。
為什麼 LOCO (Leave-One-Chromosome-Out) 資料切分如此關鍵？
在基因組預測任務中，尤其是像 CTCF 結合預測這類問題，最關鍵的挑戰在於確保模型能夠泛化到從未見過的基因組區域 [Schreiber et al. 2020, 4]。
•
資訊洩漏與過度高估：
◦
README.md 檔中描述的「隨機序列切分」策略，若未明確排除基因組區域重疊，那麼模型性能仍可能被過度高估 [LOCO-EPI 2024, 473, 478, 492, 499]。
◦
這是因為模型可能「記住」了訓練集和測試集共有的基因組位點特徵，而非真正學習到泛化的生物學規律 [Schreiber et al. 2020, 2, 7, 10, LOCO-EPI 2024, 473, 478, 492, 499]。
◦
源文獻指出，在隨機分割下表現出 0.99 AUCROC 的模型，在 LOCO 設定下性能會大幅下降至接近隨機水準 (AUC ≈ 0.5)，這表明了性能的嚴重過度高估 [LOCO-EPI 2024, 478, 492, 499]。
•
CTCF 特性：
◦
CTCF 作為一個在不同細胞類型中具有相似結合模式的轉錄因子，特別容易受到這種「記憶」效應的影響 [Schreiber et al. 2020, 8, 11]。這使得採用更嚴格的評估方法變得尤為重要。
•
真實泛化能力：
◦
LOCO 策略通過將整條染色體保留作為測試集來完全避免訓練和測試集之間的基因組區域重疊 [Schreiber et al. 2020, 4, LOCO-EPI 2024, 473, 505]。
◦
這確保了模型能夠泛化到從未見過的基因組區域，從而提供對其真實泛化能力的公正評估 [Schreiber et al.2020, 4, LOCO-EPI 2024, 473, 479]。
最佳實踐與未來優化方向
•
定量模型為佳：優先使用能夠預測基於序列的實際讀取覆蓋值的定量模型（如 BPNet），而非僅預測二元結合事件 [Avsec et al. 2021, 39, 42, 55, 99, 100, Toneyan et al. 2023, 361, 376]。這些模型保留了更多的生物學資訊 [Toneyan et al. 2023, 361]。
•
混合模型：考慮在深度學習模型中結合傳統特徵（如 k-mer 頻率），這有助於提高泛化能力並防止過度擬合 [LOCO-EPI 2024, 473, 479, 486, 487, 488, 489, 494, 497, 498, 499, 500, 501, 502, 504]。
•
資料增強：在訓練時使用隨機平移和反向互補等資料增強技術，以提高模型對輸入變化的魯棒性 [Toneyan et al. 2023, 373, 374, 391, 409, 410, 411, 413, 462, 463, 467, 468]。
•
解釋性工具：積極利用模型解釋性工具（如 DeepLIFT、TF-MoDISco、GIA），以從訓練好的模型中提取生物學上有意義的 motif 和語法規則 [Avsec et al. 2021, 35, 36, 44, 47, 50, 58, 66, 67, 89, 93, 94, Toneyan et al. 2023, 380, 391, 421, 422, 423]。
•
多任務學習：如果數據允許，可同時預測多個轉錄因子或細胞類型的結合，這已被證明能提高模型的效率和泛化能力 [NetTIME 2022, 530, 534, 538, 539, 540, 559, 560, 561, 566]。
•
更長序列上下文：使用 1kb 或更長的輸入序列，因為序列上下文對模型性能有顯著影響 [Avsec et al. 2021, 40, 220, Zhou & Troyanskaya 2015, 621]。